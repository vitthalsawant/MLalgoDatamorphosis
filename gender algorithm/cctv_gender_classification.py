# -*- coding: utf-8 -*-
"""CCTV Gender Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/vitthalsawant/cctv-gender-classification.8433a6fe-f130-4f01-a86b-5c6c13600e8a.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251213/auto/storage/goog4_request%26X-Goog-Date%3D20251213T182925Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6b373e34eca91d34a7af3efcd05ee26c0c4185cf92c55f7c58e1400a6573703f6a54218ba8936ec74089fab046567803914ef2aee95a43f6b5d72e54989c1cd120a7204c827e50250bba771e0e7a278b185cfdf9700b737b18a27613af4e7331c54e35f8d1208ccbf1eb7022526ea0dcc5ed264ebc1f8673953aac4f9a75a07812f29c0243ca15d90e3d5f318d2b624a4e4657239f97e552705c63d299be5bc2866ffd9d4e76cf74202a272eec853b8ce9bfc66f4184b957a01dc017166e2fd73aa6e73edc57c4023407a69c3783e4b103a7c4ecfc2ff4985ae283ba2067f2f31d3ba64b7a175a1a792769779b9b7a209a1c1c27938eee9237d12a746f3c2546
"""

# Dataset paths updated to use local Windows paths
print('Data source import complete.')

import os
import random
import shutil
import glob as gb
from tqdm import tqdm
from collections import Counter
import cv2
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
from PIL import Image, ImageTk
import threading

import numpy as np
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend to avoid tkinter conflicts
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns

from tensorflow.keras.preprocessing.image import load_img,img_to_array
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, AveragePooling2D,Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.models import Model, load_model

from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix

def visualize_random_images(male_folder, female_folder, num_images=5):
    male_images = os.listdir(male_folder)
    female_images = os.listdir(female_folder)

    fig, axes = plt.subplots(2, num_images, figsize=(15, 7))
    plt.subplots_adjust(wspace=0.3, hspace=0.3)

    for i in range(num_images):
        male_image_path = os.path.join(male_folder, random.choice(male_images))
        female_image_path = os.path.join(female_folder, random.choice(female_images))

        male_img = plt.imread(male_image_path)
        female_img = plt.imread(female_image_path)

        axes[0, i].imshow(male_img)
        axes[0, i].axis('off')
        axes[0, i].set_title('Male')

        axes[1, i].imshow(female_img)
        axes[1, i].axis('off')
        axes[1, i].set_title('Female')

    plt.savefig('sample_images.png')
    plt.close()

# Updated paths to use local Windows directories
male_folder = r'C:\Users\vitth\Downloads\CCTV Gender Classifier Dataset\CCTV Gender Classifier Dataset\MALE'
female_folder = r'C:\Users\vitth\Downloads\CCTV Gender Classifier Dataset\CCTV Gender Classifier Dataset\FEMALE'

visualize_random_images(male_folder, female_folder)

num_male_images = len(os.listdir(male_folder))
num_female_images = len(os.listdir(female_folder))

print(f"Total number of male images: {num_male_images}")
print(f"Total number of female images: {num_female_images}")

# Create bar plot
categories = ['Male', 'Female']
num_images = [num_male_images, num_female_images]

plt.bar(categories, num_images, color=['blue', 'pink'])
plt.xlabel('Category')
plt.ylabel('Number of Images')
plt.title('Number of Images in Each Category')
plt.savefig('dataset_distribution.png')
plt.close()  # Close instead of show to avoid tkinter conflicts

DataPath = r'C:\Users\vitth\Downloads\CCTV Gender Classifier Dataset\CCTV Gender Classifier Dataset'

labels2int={"MALE":0,"FEMALE":1}
int2labels={0:"MALE",1:"FEMALE"}

Data = []
Classes = []

# Use only a small subset of data for faster training (20% of available data)
USE_SMALL_DATASET = True
SUBSET_PERCENTAGE = 0.2  # Use only 20% of data

# Iterate through each folder in DataPath
for folder in os.listdir(DataPath):
    print(folder)

    # Construct the full path to the folder
    folder_path = os.path.join(DataPath, folder)

    # Get a list of all files in the folder
    files = os.listdir(folder_path)
    
    # Limit the number of files if using small dataset
    if USE_SMALL_DATASET:
        num_files_to_use = max(1, int(len(files) * SUBSET_PERCENTAGE))
        files = random.sample(files, min(num_files_to_use, len(files)))
        print(f"Using {len(files)} out of {len(os.listdir(folder_path))} files from {folder} (small dataset mode)")

    print(f"Data found {len(files)} in {folder}")

    # Iterate through each file in the folder
    for file in files:
        # Construct the full path to the file
        file_path = os.path.join(folder_path, file)

        # Load the image, resize it, and convert it to an array
        img = load_img(file_path, target_size=(200, 100))
        img_array = img_to_array(img)

        # Preprocess the image array
        img_array = preprocess_input(img_array)

        # Append the preprocessed image array to the Data list
        Data.append(img_array)

        # Append the label to the Classes list based on the folder name
        Classes.append(labels2int[folder])

print("Length of Classes before binarization:", len(Classes))

lb = LabelBinarizer()
Classes = lb.fit_transform(Classes)
Classes = to_categorical(Classes)

print("Shape of Classes after binarization and conversion:", Classes.shape)

Data = np.array(Data,dtype = "float32")
Classes = np.array(Classes)
# Split data into training and testing sets (80% train, 20% test)
Data_train, Data_test, Classes_train, Classes_test = train_test_split(Data, Classes, test_size=0.2, stratify=Classes, random_state=42, shuffle=True)

# Split training data into training and validation sets (80% train, 20% validation)
Data_train, Data_val, Classes_train, Classes_val = train_test_split(Data_train, Classes_train, test_size=0.2, stratify=Classes_train, random_state=42)

# Free up memory by deleting the original Data array
del Data

print("Shape of training data (Data_train):", Data_train.shape)
print("Shape of Validation data (Data_val):", Data_val.shape)
print("Shape of testing data (Data_test):", Data_test.shape)
print('=============================================================')
print("Shape of training labels (Classes_train):", Classes_train.shape)
print("Shape of Validation labels (Classes_val):", Classes_val.shape)
print("Shape of testing labels (Classes_test):", Classes_test.shape)

def resnet50_modelarch():
    lr = 1e-5
    epochs = 10

    basemodel= ResNet50(include_top=False, input_shape=(200,100,3))
    headmodel= basemodel.output
    headmodel= AveragePooling2D(pool_size=(3,3))(headmodel)

    #headmodel = Dense(1024, activation="relu")(headmodel)
    #headmodel = Dropout(0.3)(headmodel)
    headmodel = Flatten(name="flatten")(headmodel)
    headmodel = Dense(512, activation="relu")(headmodel)
    headmodel = Dropout(0.3)(headmodel)
    headmodel = Dense(256, activation="relu")(headmodel)
    headmodel = Dropout(0.3)(headmodel)
    headmodel = Dense(128, activation="relu")(headmodel)
    headmodel = Dropout(0.3)(headmodel)
    headmodel = Dense(64, activation="relu")(headmodel)
    #headmodel = Dropout(0.3)(headmodel)
    headmodel = Dense(2, activation='softmax')(headmodel)

    model=Model(inputs=basemodel.input, outputs=headmodel)

    for layer in basemodel.layers:
        layer.trainable=False

    opt=Adam(learning_rate=lr, decay=lr / epochs)
    model.compile(loss="binary_crossentropy",optimizer=opt,metrics=["accuracy"])
    callback = EarlyStopping(monitor='val_loss',patience=6)

    return model,callback,epochs

model,callback,Epochs = resnet50_modelarch()
model.summary()

history = model.fit(Data_train,Classes_train,
                    batch_size=32,
                    validation_data=(Data_val,Classes_val),
                    epochs=10,callbacks=[callback])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('acc')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validate'], loc='upper left')
plt.savefig('model_accuracy.png')
plt.close()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validate'], loc='upper left')
plt.savefig('model_loss.png')
plt.close()

scores = model.evaluate(Data_test,Classes_test, verbose=1)
print("ResNet50 Score:",scores[1])

predIdxs = model.predict(Data_test,batch_size=32)
predIdxs=np.argmax(predIdxs,axis=1)

from sklearn.metrics import classification_report,confusion_matrix
testy_res = Classes_test.argmax(axis=1)
CR = classification_report(testy_res, predIdxs)
print(CR)

# Get predictions for the test set
predictions = model.predict(Data_test)

# Convert predictions to class labels (0 for male, 1 for female)
predicted_labels = np.argmax(predictions, axis=1)

# Compare predicted labels with true labels
correct_predictions = np.sum(predicted_labels == np.argmax(Classes_test, axis=1))
total_samples = len(Classes_test)

# Calculate accuracy
accuracy = correct_predictions / total_samples
print("Test Accuracy:", accuracy)

class_names=['MALE','FEMALE']

import random
def plot_prediction(Data_test, Classes_test, n_images, class_names):
    """
    Test the model on random predictions and plot the results.

    Args:
        Data_test (numpy array): The test images data.
        Classes_test (numpy array): The true labels for the test images.
        n_images (int): Number of images to plot.
        class_names (list): List of class names.
    """
    # Get the total number of test images
    num_images = len(Data_test)

    # Randomly select indices for the images to plot
    random_indices = random.sample(range(num_images), n_images)

    # Make predictions on the selected test data
    predictions = np.argmax(model.predict(Data_test[random_indices]), axis=1)

    # Plot the randomly selected images along with their true labels and predictions
    plt.figure(figsize=(14, 15))
    for i, idx in enumerate(random_indices):
        plt.subplot(4, 3, i+1)
        plt.imshow(Data_test[idx])
        if predictions[i] == np.argmax(Classes_test[idx]):
            title_color = 'g'  # green color for correct predictions
        else:
            title_color = 'r'  # red color for incorrect predictions
        plt.title(class_names[np.argmax(Classes_test[idx])], color=title_color)
        plt.axis('off')

    plt.savefig('prediction_samples.png')
    plt.close()

plot_prediction(Data_test, Classes_test, n_images=6, class_names=class_names)

model.save("GenderClassification.h5")

# ============================================================================
# CCTV Footage Analysis GUI Application
# ============================================================================

class CCTVGenderAnalyzer:
    def __init__(self, root):
        self.root = root
        self.root.title("CCTV Gender Classification Analyzer")
        self.root.geometry("800x600")
        self.root.configure(bg='#f0f0f0')
        
        self.model = None
        self.face_cascade = None
        self.video_path = None
        self.last_results = None
        
        # Try to load saved model
        self.load_model()
        self.load_face_cascade()
        
        self.setup_ui()
        
    def load_model(self):
        """Load the trained gender classification model"""
        try:
            if os.path.exists("GenderClassification.h5"):
                print("Loading saved model...")
                self.model = load_model("GenderClassification.h5")
                print("Model loaded successfully!")
            else:
                print("No saved model found. Checking for model in memory...")
                # Check if model exists in global scope (from training)
                if 'model' in globals() and model is not None:
                    print("Using the trained model from memory.")
                    self.model = model
                else:
                    print("WARNING: No model available. Please train the model first.")
                    self.model = None
        except Exception as e:
            print(f"Error loading model: {e}")
            import traceback
            traceback.print_exc()
            # Try to use model from memory as fallback
            if 'model' in globals() and model is not None:
                print("Using model from memory as fallback.")
                self.model = model
            else:
                self.model = None
    
    def load_face_cascade(self):
        """Load OpenCV face detection cascade"""
        try:
            # Try to load the cascade file
            cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            self.face_cascade = cv2.CascadeClassifier(cascade_path)
            if self.face_cascade.empty():
                raise Exception("Could not load face cascade")
        except Exception as e:
            print(f"Error loading face cascade: {e}")
            messagebox.showerror("Error", "Could not load face detection model. Please check OpenCV installation.")
    
    def setup_ui(self):
        """Setup the user interface"""
        # Title
        title_label = tk.Label(self.root, text="CCTV Gender Classification", 
                              font=("Arial", 20, "bold"), bg='#f0f0f0')
        title_label.pack(pady=20)
        
        # Instructions
        instruction_label = tk.Label(self.root, 
                                    text="Upload a CCTV video file to analyze gender distribution",
                                    font=("Arial", 12), bg='#f0f0f0')
        instruction_label.pack(pady=10)
        
        # Upload button
        upload_btn = tk.Button(self.root, text="Select CCTV Video File", 
                               command=self.select_video, font=("Arial", 12),
                               bg='#4CAF50', fg='white', padx=20, pady=10,
                               cursor='hand2')
        upload_btn.pack(pady=20)
        
        # Video path label
        self.video_path_label = tk.Label(self.root, text="No video selected", 
                                         font=("Arial", 10), bg='#f0f0f0', 
                                         wraplength=700)
        self.video_path_label.pack(pady=10)
        
        # Analyze button
        self.analyze_btn = tk.Button(self.root, text="Analyze Video", 
                                     command=self.analyze_video, font=("Arial", 12),
                                     bg='#2196F3', fg='white', padx=20, pady=10,
                                     cursor='hand2', state='disabled')
        self.analyze_btn.pack(pady=10)
        
        # Progress bar
        self.progress = ttk.Progressbar(self.root, mode='indeterminate')
        self.progress.pack(pady=10, padx=50, fill='x')
        
        # Results frame
        results_frame = tk.Frame(self.root, bg='#f0f0f0')
        results_frame.pack(pady=20, padx=50, fill='both', expand=True)
        
        # Results title
        results_title = tk.Label(results_frame, text="Analysis Results", 
                                font=("Arial", 14, "bold"), bg='#f0f0f0')
        results_title.pack(pady=10)
        
        # Results display
        self.results_text = tk.Text(results_frame, height=10, width=60, 
                                    font=("Arial", 11), wrap=tk.WORD,
                                    bg='white', relief=tk.SUNKEN, borderwidth=2)
        self.results_text.pack(pady=10, fill='both', expand=True)
        
        scrollbar = tk.Scrollbar(self.results_text)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.results_text.config(yscrollcommand=scrollbar.set)
        scrollbar.config(command=self.results_text.yview)
    
    def select_video(self):
        """Open file dialog to select video file"""
        file_path = filedialog.askopenfilename(
            title="Select CCTV Video File",
            filetypes=[
                ("Video files", "*.mp4 *.avi *.mov *.mkv *.flv *.wmv"),
                ("All files", "*.*")
            ]
        )
        
        if file_path:
            self.video_path = file_path
            self.video_path_label.config(text=f"Selected: {os.path.basename(file_path)}")
            self.analyze_btn.config(state='normal')
    
    def detect_faces(self, frame):
        """Detect faces in a frame"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(30, 30)
        )
        return faces
    
    def preprocess_face(self, face_roi):
        """Preprocess face ROI for gender classification"""
        # Resize to match model input size (200, 100) = (height, width)
        # cv2.resize takes (width, height), so we use (100, 200)
        face_resized = cv2.resize(face_roi, (100, 200))
        # Convert BGR to RGB
        face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)
        # Preprocess for ResNet50
        face_array = preprocess_input(face_rgb)
        # Expand dimensions for batch
        face_array = np.expand_dims(face_array, axis=0)
        return face_array
    
    def analyze_video(self):
        """Analyze video frames for gender classification"""
        if not self.video_path:
            messagebox.showwarning("Warning", "Please select a video file first.")
            return
        
        if not self.model:
            messagebox.showerror("Error", "Model not available. Please train the model first.\n\nMake sure 'GenderClassification.h5' exists or run the training code.")
            return
        
        if not self.face_cascade or self.face_cascade.empty():
            messagebox.showerror("Error", "Face detection model not available.\n\nPlease check OpenCV installation.")
            return
        
        # Disable button and start progress bar
        self.analyze_btn.config(state='disabled')
        self.progress.start()
        self.results_text.delete(1.0, tk.END)
        self.results_text.insert(tk.END, "Analyzing video... Please wait.\n\n")
        
        # Reset results
        self.last_results = None
        
        # Run analysis in separate thread to prevent UI freezing
        thread = threading.Thread(target=self._analyze_video_thread, daemon=True)
        thread.start()
    
    def _analyze_video_thread(self):
        """Analyze video in background thread"""
        cap = None
        try:
            cap = cv2.VideoCapture(self.video_path)
            
            if not cap.isOpened():
                raise Exception("Could not open video file")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS) or 30.0  # Default to 30 if FPS is 0
            
            # Process every Nth frame to speed up analysis
            frame_skip = max(1, int(fps / 2))  # Process 2 frames per second
            
            male_count = 0
            female_count = 0
            total_faces_detected = 0
            frames_processed = 0
            
            # Store video info for display
            video_info = f"Video Info:\n- Total frames: {total_frames}\n- FPS: {fps:.2f}\n- Processing every {frame_skip} frame(s)\n\n"
            if self.root.winfo_exists():
                self.root.after(0, self._update_results, video_info)
            
            frame_idx = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Process every Nth frame
                if frame_idx % frame_skip == 0:
                    faces = self.detect_faces(frame)
                    
                    for (x, y, w, h) in faces:
                        # Extract face ROI
                        face_roi = frame[y:y+h, x:x+w]
                        
                        # Skip if face ROI is too small
                        if face_roi.size == 0 or w < 30 or h < 30:
                            continue
                        
                        try:
                            # Preprocess and predict
                            face_array = self.preprocess_face(face_roi)
                            prediction = self.model.predict(face_array, verbose=0)
                            gender_idx = np.argmax(prediction[0])
                            
                            if gender_idx == 0:  # MALE
                                male_count += 1
                            else:  # FEMALE
                                female_count += 1
                            
                            total_faces_detected += 1
                        except Exception as e:
                            print(f"Error processing face: {e}")
                            continue
                    
                    frames_processed += 1
                    
                    # Update progress every 10 frames
                    if frames_processed % 10 == 0 and self.root.winfo_exists():
                        progress_text = f"Processed {frames_processed} frames... Detected {total_faces_detected} faces so far...\n"
                        self.root.after(0, self._update_results, progress_text)
                
                frame_idx += 1
            
            if cap:
                cap.release()
            
            # Final results
            total_people = male_count + female_count
            male_percentage = (male_count / total_people * 100) if total_people > 0 else 0
            female_percentage = (female_count / total_people * 100) if total_people > 0 else 0
            
            video_filename = os.path.basename(self.video_path) if self.video_path else "Unknown"
            
            results = f"""
{'='*60}
ANALYSIS COMPLETE
{'='*60}

Video File: {video_filename}

Video Statistics:
- Total frames in video: {total_frames}
- Frames processed: {frames_processed}
- Total faces detected: {total_faces_detected}

Gender Distribution:
- Male Count: {male_count} ({male_percentage:.2f}%)
- Female Count: {female_count} ({female_percentage:.2f}%)
- Total People: {total_people}

{'='*60}
"""
            
            # Store results in instance variable for access
            self.last_results = {
                'male_count': male_count,
                'female_count': female_count,
                'total_people': total_people,
                'total_faces': total_faces_detected,
                'frames_processed': frames_processed,
                'total_frames': total_frames
            }
            
            if self.root.winfo_exists():
                self.root.after(0, self._show_final_results, results)
            
        except Exception as e:
            import traceback
            error_msg = f"Error analyzing video: {str(e)}"
            print(f"Error in video analysis thread: {error_msg}")
            print(traceback.format_exc())
            if self.root.winfo_exists():
                self.root.after(0, self._show_error, error_msg)
        finally:
            if cap:
                cap.release()
    
    def _update_results(self, text):
        """Update results text (thread-safe)"""
        try:
            if hasattr(self, 'results_text') and self.results_text.winfo_exists():
                self.results_text.insert(tk.END, text)
                self.results_text.see(tk.END)
        except (tk.TclError, AttributeError, RuntimeError):
            # GUI was closed or object doesn't exist, ignore
            pass
    
    def _show_final_results(self, results):
        """Show final results (thread-safe)"""
        try:
            if not self.root.winfo_exists():
                return
                
            self.progress.stop()
            self.results_text.delete(1.0, tk.END)
            self.results_text.insert(tk.END, results)
            self.analyze_btn.config(state='normal')
            
            # Create a formatted message for the popup
            if self.last_results:
                popup_msg = f"""Video analysis completed successfully!

Results Summary:
- Male Count: {self.last_results['male_count']}
- Female Count: {self.last_results['female_count']}
- Total People: {self.last_results['total_people']}
- Faces Detected: {self.last_results['total_faces']}
- Frames Processed: {self.last_results['frames_processed']}

See detailed results in the text area below."""
            else:
                popup_msg = "Video analysis completed successfully!\n\nSee results in the text area below."
            
            # Use after to ensure messagebox is shown after GUI update
            def show_popup():
                try:
                    if self.root.winfo_exists():
                        messagebox.showinfo("Analysis Complete", popup_msg)
                except:
                    pass
            
            self.root.after(100, show_popup)
        except (tk.TclError, AttributeError, RuntimeError):
            # GUI was closed, ignore
            pass
        except Exception as e:
            print(f"Error showing results: {e}")
    
    def _show_error(self, error_msg):
        """Show error message (thread-safe)"""
        try:
            if not self.root.winfo_exists():
                return
                
            self.progress.stop()
            self.results_text.delete(1.0, tk.END)
            self.results_text.insert(tk.END, f"ERROR: {error_msg}")
            self.analyze_btn.config(state='normal')
            
            # Use after to ensure messagebox is shown after GUI update
            def show_error_popup():
                try:
                    if self.root.winfo_exists():
                        messagebox.showerror("Error", error_msg)
                except:
                    pass
            
            self.root.after(100, show_error_popup)
        except (tk.TclError, AttributeError, RuntimeError):
            # GUI was closed, ignore
            pass

# Run the GUI application
if __name__ == "__main__":
    print("\n" + "="*60)
    print("Training completed! Starting CCTV Gender Classification Analyzer GUI...")
    print("="*60 + "\n")
    
    # Ensure matplotlib backend is set before GUI starts
    matplotlib.use('TkAgg')  # Switch to TkAgg for GUI compatibility
    
    try:
        root = tk.Tk()
        app = CCTVGenderAnalyzer(root)
        
        def on_closing():
            """Handle window closing properly"""
            try:
                root.quit()
                root.destroy()
            except:
                pass
        
        root.protocol("WM_DELETE_WINDOW", on_closing)
        root.mainloop()
    except KeyboardInterrupt:
        print("\nGUI closed by user.")
    except Exception as e:
        print(f"Error starting GUI: {e}")
        import traceback
        traceback.print_exc()

"""# Task
Adapt the existing notebook to use the "CCTV Gender Classifier Dataset" from Google Drive, located at `/content/drive/MyDrive/ML Datamorphosis/CCTV Gender Classifier Dataset.zip`, by unzipping it to a local directory and updating the data paths in the notebook to reflect this change.

## Remove Kaggle Download

### Subtask:
Remove the `kagglehub.dataset_download` code from the initial cell as we will be using data from Google Drive instead.

**Reasoning**:
The subtask is to remove the kagglehub.dataset_download line. I will modify the first code cell to remove this line, as data will be sourced from Google Drive.
"""

# Dataset paths have been updated to use local Windows directories